# -*- coding: utf-8 -*-
"""Data-augmentation for nlp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZnJZv0NWvQVhiG5oXjCbwbX8dCtl7GcP

Define all the augmented functions based on character level
"""

"""# Example of textual augmenter usage"""
from flair.data import Sentence, Corpus
from flair.datasets import SentenceDataset
from typing import List

import nlpaug
from flair.data import  Token

"""### Config"""

import nlpaug.augmenter.char as nac
import nlpaug.augmenter.word as naw
import nlpaug.augmenter.sentence as nas
import nlpaug.flow as nafc

from nlpaug.util import Action

"""### Punctuation Augmenter"""

def punctuation_aug(corpus):
    augmented_sentences = []

    # go through all train and dev sentences
    for sentence in corpus.train:
        punc = '''!()-[]{};:'"\, <>./?@#$%^&*_~''' #define the set of pucntuation that needed to be remove
        augmented_sentence: Sentence = Sentence()
        for token in sentence:
            if token.text not in punc:
                augmented_sentence.add_token(token)

                # append to augmented sentences
        if len(augmented_sentence) > 0:
            augmented_sentences.append(augmented_sentence)

    # make a new corpus with the augmented sentences
    corpus = Corpus(train=SentenceDataset(augmented_sentences),
                    dev=corpus.dev,
                    test=corpus.test)
    return corpus


"""### capitalization Augmenter"""

def capitalization_aug(corpus):
    augmented_sentences = []

    # go through all train and dev sentences
    for sentence in corpus.train:
        augmented_sentence: Sentence = Sentence()
        for token in sentence:
            token.text = token.text.lower()
            augmented_sentence.add_token(token)


                # append to augmented sentences
        if len(augmented_sentence) > 0:
            augmented_sentences.append(augmented_sentence)

    # make a new corpus with the augmented sentences
    corpus = Corpus(train=SentenceDataset(augmented_sentences),
                    dev=corpus.dev,
                    test=corpus.test)
    return corpus

"""### Character Augmenter

#### OCR Augmenter
"""

def whitespace_tokenizer(text):
    tokens = text.split(" ")
    return [t for t in tokens if len(t.strip()) > 0]


def ocr_aug(corpus):
    aug = nac.OcrAug(tokenizer=whitespace_tokenizer)
    # go through all train and dev sentences
    augmented_sentences = []
    for sentence in corpus.train:
        augmented_texts = aug.augment(sentence.to_tokenized_string(), n=3)
        for augmented_text in augmented_texts:
            augmented_sentence: Sentence = Sentence()
            augmented_token_texts = augmented_text.split(" ")
            for augmented_token_text, original_token in zip(augmented_token_texts, sentence):
                # make a new token
                augmented_token = Token(augmented_token_text)
                # transfer annotations over to augmented token
                augmented_token.annotation_layers = original_token.annotation_layers
                # add augmented token to augmented sentence
                augmented_sentence.add_token(augmented_token)
            # add augmented sentence to list of all augmented sentences
            augmented_sentences.append(augmented_sentence)

    corpus = Corpus(train=SentenceDataset(augmented_sentences),
                    dev=corpus.dev,
                    test=corpus.test)

    return corpus

"""#### Keyboard Augmenter"""

def keyboard_aug(corpus):
    aug = nac.KeyboardAug(tokenizer=whitespace_tokenizer)
    # go through all train and dev sentences
    augmented_sentences = []
    for sentence in corpus.train:
        augmented_texts = aug.augment(sentence.to_tokenized_string(), n=3)
        for augmented_text in augmented_texts:
            augmented_sentence: Sentence = Sentence()
            augmented_token_texts = augmented_text.split(" ")
            for augmented_token_text, original_token in zip(augmented_token_texts, sentence):
                # make a new token
                augmented_token = Token(augmented_token_text)
                # transfer annotations over to augmented token
                augmented_token.annotation_layers = original_token.annotation_layers
                # add augmented token to augmented sentence
                augmented_sentence.add_token(augmented_token)
            # add augmented sentence to list of all augmented sentences
            augmented_sentences.append(augmented_sentence)

    corpus = Corpus(train=SentenceDataset(augmented_sentences),
                    dev=corpus.dev,
                    test=corpus.test)

    return corpus


"""#### Random Augmenter"""
#### insertion

def random_insert_aug(corpus):
    aug = nac.RandomCharAug(tokenizer=whitespace_tokenizer, action="insert")
    # go through all train and dev sentences
    augmented_sentences = []
    for sentence in corpus.train:
        augmented_texts = aug.augment(sentence.to_tokenized_string(), n=3)
        for augmented_text in augmented_texts:
            augmented_sentence: Sentence = Sentence()
            augmented_token_texts = augmented_text.split(" ")
            for augmented_token_text, original_token in zip(augmented_token_texts, sentence):
                # make a new token
                augmented_token = Token(augmented_token_text)
                # transfer annotations over to augmented token
                augmented_token.annotation_layers = original_token.annotation_layers
                # add augmented token to augmented sentence
                augmented_sentence.add_token(augmented_token)
            # add augmented sentence to list of all augmented sentences
            augmented_sentences.append(augmented_sentence)

    corpus = Corpus(train=SentenceDataset(augmented_sentences),
                    dev=corpus.dev,
                    test=corpus.test)

    return corpus


#### substitution

def random_subtitute_aug(corpus):
    aug = nac.RandomCharAug(tokenizer=whitespace_tokenizer, action="substitute")
    # go through all train and dev sentences
    augmented_sentences = []
    for sentence in corpus.train:
        augmented_texts = aug.augment(sentence.to_tokenized_string(), n=3)
        for augmented_text in augmented_texts:
            augmented_sentence: Sentence = Sentence()
            augmented_token_texts = augmented_text.split(" ")
            for augmented_token_text, original_token in zip(augmented_token_texts, sentence):
                # make a new token
                augmented_token = Token(augmented_token_text)
                # transfer annotations over to augmented token
                augmented_token.annotation_layers = original_token.annotation_layers
                # add augmented token to augmented sentence
                augmented_sentence.add_token(augmented_token)
            # add augmented sentence to list of all augmented sentences
            augmented_sentences.append(augmented_sentence)

    corpus = Corpus(train=SentenceDataset(augmented_sentences),
                    dev=corpus.dev,
                    test=corpus.test)

    return corpus


#### swaping

def random_swap_aug(corpus):
    aug = nac.RandomCharAug(tokenizer=whitespace_tokenizer, action="swap")
    # go through all train and dev sentences
    augmented_sentences = []
    for sentence in corpus.train:
        augmented_texts = aug.augment(sentence.to_tokenized_string(), n=3)
        for augmented_text in augmented_texts:
            augmented_sentence: Sentence = Sentence()
            augmented_token_texts = augmented_text.split(" ")
            for augmented_token_text, original_token in zip(augmented_token_texts, sentence):
                # make a new token
                augmented_token = Token(augmented_token_text)
                # transfer annotations over to augmented token
                augmented_token.annotation_layers = original_token.annotation_layers
                # add augmented token to augmented sentence
                augmented_sentence.add_token(augmented_token)
            # add augmented sentence to list of all augmented sentences
            augmented_sentences.append(augmented_sentence)

    corpus = Corpus(train=SentenceDataset(augmented_sentences),
                    dev=corpus.dev,
                    test=corpus.test)

    return corpus

#### deletion

def random_delete_aug(corpus):
    aug = nac.RandomCharAug(tokenizer=whitespace_tokenizer, action="delete")
    # go through all train and dev sentences
    augmented_sentences = []
    for sentence in corpus.train:
        augmented_texts = aug.augment(sentence.to_tokenized_string(), n=3)
        for augmented_text in augmented_texts:
            augmented_sentence: Sentence = Sentence()
            augmented_token_texts = augmented_text.split(" ")
            for augmented_token_text, original_token in zip(augmented_token_texts, sentence):
                # make a new token
                augmented_token = Token(augmented_token_text)
                # transfer annotations over to augmented token
                augmented_token.annotation_layers = original_token.annotation_layers
                # add augmented token to augmented sentence
                augmented_sentence.add_token(augmented_token)
            # add augmented sentence to list of all augmented sentences
            augmented_sentences.append(augmented_sentence)

    corpus = Corpus(train=SentenceDataset(augmented_sentences),
                    dev=corpus.dev,
                    test=corpus.test)

    return corpus

