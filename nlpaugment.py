# -*- coding: utf-8 -*-
"""Data-augmentation for nlp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZnJZv0NWvQVhiG5oXjCbwbX8dCtl7GcP
"""
# import nlpaug.util.file.download
# from nlpaug.util.file.download import DownloadUtil

"""# Example of textual augmenter usage"""
from flair.data import Sentence, Corpus
from flair.datasets import CONLL_03, SentenceDataset
from typing import List

import nlpaug
from flair.data import  Token

"""### Config"""

import nlpaug.augmenter.char as nac
import nlpaug.augmenter.word as naw
import nlpaug.augmenter.sentence as nas
import nlpaug.flow as nafc

from nlpaug.util import Action


def punctuation_aug(corpus):
    augmented_sentences = []

    # go through all train and dev sentences
    for sentence in corpus.train:
        punc = '''!()-[]{};:'"\, <>./?@#$%^&*_~'''
        augmented_sentence: Sentence = Sentence()
        for token in sentence:
            if token.text not in punc:
                augmented_sentence.add_token(token)

                # append to augmented sentences
        if len(augmented_sentence) > 0:
            augmented_sentences.append(augmented_sentence)

    # make a new corpus with the augmented sentences
    corpus = Corpus(train=SentenceDataset(augmented_sentences),
                    dev=corpus.dev,
                    test=corpus.test)
    return corpus


def capitalization_aug(corpus):
    augmented_sentences = []

    # go through all train and dev sentences
    for sentence in corpus.train:
        augmented_sentence: Sentence = Sentence()
        for token in sentence:
            token.text = token.text.lower()
            augmented_sentence.add_token(token)


                # append to augmented sentences
        if len(augmented_sentence) > 0:
            augmented_sentences.append(augmented_sentence)

    # make a new corpus with the augmented sentences
    corpus = Corpus(train=SentenceDataset(augmented_sentences),
                    dev=corpus.dev,
                    test=corpus.test)
    return corpus

"""### Character Augmenter

#### OCR Augmenter
"""

def whitespace_tokenizer(text):
    tokens = text.split(" ")
    return [t for t in tokens if len(t.strip()) > 0]


def ocr_aug(corpus):
    aug = nac.OcrAug(tokenizer=whitespace_tokenizer)
    # go through all train and dev sentences
    augmented_sentences = []
    for sentence in corpus.train:
        augmented_texts = aug.augment(sentence.to_tokenized_string(), n=3)
        for augmented_text in augmented_texts:
            augmented_sentence: Sentence = Sentence()
            augmented_token_texts = augmented_text.split(" ")
            for augmented_token_text, original_token in zip(augmented_token_texts, sentence):
                # make a new token
                augmented_token = Token(augmented_token_text)
                # transfer annotations over to augmented token
                augmented_token.annotation_layers = original_token.annotation_layers
                # add augmented token to augmented sentence
                augmented_sentence.add_token(augmented_token)
            # add augmented sentence to list of all augmented sentences
            augmented_sentences.append(augmented_sentence)

    corpus = Corpus(train=SentenceDataset(augmented_sentences),
                    dev=corpus.dev,
                    test=corpus.test)

    return corpus

"""#### Keyboard Augmenter"""

def keyboard_aug(corpus):
    aug = nac.KeyboardAug(tokenizer=whitespace_tokenizer)
    # go through all train and dev sentences
    augmented_sentences = []
    for sentence in corpus.train:
        augmented_texts = aug.augment(sentence.to_tokenized_string(), n=3)
        for augmented_text in augmented_texts:
            augmented_sentence: Sentence = Sentence()
            augmented_token_texts = augmented_text.split(" ")
            for augmented_token_text, original_token in zip(augmented_token_texts, sentence):
                # make a new token
                augmented_token = Token(augmented_token_text)
                # transfer annotations over to augmented token
                augmented_token.annotation_layers = original_token.annotation_layers
                # add augmented token to augmented sentence
                augmented_sentence.add_token(augmented_token)
            # add augmented sentence to list of all augmented sentences
            augmented_sentences.append(augmented_sentence)

    corpus = Corpus(train=SentenceDataset(augmented_sentences),
                    dev=corpus.dev,
                    test=corpus.test)

    return corpus


"""#### Random Augmenter"""

def random_insert_aug(corpus):
    aug = nac.RandomCharAug(tokenizer=whitespace_tokenizer, action="insert")
    # go through all train and dev sentences
    augmented_sentences = []
    for sentence in corpus.train:
        augmented_texts = aug.augment(sentence.to_tokenized_string(), n=3)
        for augmented_text in augmented_texts:
            augmented_sentence: Sentence = Sentence()
            augmented_token_texts = augmented_text.split(" ")
            for augmented_token_text, original_token in zip(augmented_token_texts, sentence):
                # make a new token
                augmented_token = Token(augmented_token_text)
                # transfer annotations over to augmented token
                augmented_token.annotation_layers = original_token.annotation_layers
                # add augmented token to augmented sentence
                augmented_sentence.add_token(augmented_token)
            # add augmented sentence to list of all augmented sentences
            augmented_sentences.append(augmented_sentence)

    corpus = Corpus(train=SentenceDataset(augmented_sentences),
                    dev=corpus.dev,
                    test=corpus.test)

    return corpus

def random_subtitute_aug(corpus):
    aug = nac.RandomCharAug(tokenizer=whitespace_tokenizer, action="substitute")
    # go through all train and dev sentences
    augmented_sentences = []
    for sentence in corpus.train:
        augmented_texts = aug.augment(sentence.to_tokenized_string(), n=3)
        for augmented_text in augmented_texts:
            augmented_sentence: Sentence = Sentence()
            augmented_token_texts = augmented_text.split(" ")
            for augmented_token_text, original_token in zip(augmented_token_texts, sentence):
                # make a new token
                augmented_token = Token(augmented_token_text)
                # transfer annotations over to augmented token
                augmented_token.annotation_layers = original_token.annotation_layers
                # add augmented token to augmented sentence
                augmented_sentence.add_token(augmented_token)
            # add augmented sentence to list of all augmented sentences
            augmented_sentences.append(augmented_sentence)

    corpus = Corpus(train=SentenceDataset(augmented_sentences),
                    dev=corpus.dev,
                    test=corpus.test)

    return corpus

def random_swap_aug(corpus):
    aug = nac.RandomCharAug(tokenizer=whitespace_tokenizer, action="swap")
    # go through all train and dev sentences
    augmented_sentences = []
    for sentence in corpus.train:
        augmented_texts = aug.augment(sentence.to_tokenized_string(), n=3)
        for augmented_text in augmented_texts:
            augmented_sentence: Sentence = Sentence()
            augmented_token_texts = augmented_text.split(" ")
            for augmented_token_text, original_token in zip(augmented_token_texts, sentence):
                # make a new token
                augmented_token = Token(augmented_token_text)
                # transfer annotations over to augmented token
                augmented_token.annotation_layers = original_token.annotation_layers
                # add augmented token to augmented sentence
                augmented_sentence.add_token(augmented_token)
            # add augmented sentence to list of all augmented sentences
            augmented_sentences.append(augmented_sentence)

    corpus = Corpus(train=SentenceDataset(augmented_sentences),
                    dev=corpus.dev,
                    test=corpus.test)

    return corpus

def random_delete_aug(corpus):
    aug = nac.RandomCharAug(tokenizer=whitespace_tokenizer, action="delete")
    # go through all train and dev sentences
    augmented_sentences = []
    for sentence in corpus.train:
        augmented_texts = aug.augment(sentence.to_tokenized_string(), n=3)
        for augmented_text in augmented_texts:
            augmented_sentence: Sentence = Sentence()
            augmented_token_texts = augmented_text.split(" ")
            for augmented_token_text, original_token in zip(augmented_token_texts, sentence):
                # make a new token
                augmented_token = Token(augmented_token_text)
                # transfer annotations over to augmented token
                augmented_token.annotation_layers = original_token.annotation_layers
                # add augmented token to augmented sentence
                augmented_sentence.add_token(augmented_token)
            # add augmented sentence to list of all augmented sentences
            augmented_sentences.append(augmented_sentence)

    corpus = Corpus(train=SentenceDataset(augmented_sentences),
                    dev=corpus.dev,
                    test=corpus.test)

    return corpus



#### Spelling Augmenter
def spelling_aug(corpus):
    aug = naw.SpellingAug()
   # augmented_sentences = []

    # go through all train and dev sentences
    for sentence in corpus.train:
        augmented_texts = aug.augment(sentence, n=3)

    corpus = Corpus(train=SentenceDataset(augmented_texts),
                    dev=corpus.dev,
                    test=corpus.test)
    return corpus

"""### Word Augmenter

#### Spelling Augmenter
"""

# aug = naw.SpellingAug()
# augmented_texts = aug.augment(text, n=3)
# print("Original:")
# print(text)
# print("Augmented Texts:")
# print(augmented_texts)
#
# aug = naw.SpellingAug()
# augmented_texts = aug.augment(text, n=3)
# print("Original:")
# print(text)
# print("Augmented Texts:")
# print(augmented_texts)

"""#### Word Embeddings Augmenter"""

#import nlpaug.util.file.download
# DownloadUtil.download_word2vec(dest_dir='.') # Download word2vec model
# DownloadUtil.download_glove(model_name='glove.6B', dest_dir='.') # Download GloVe model
# DownloadUtil.download_fasttext(model_name='wiki-news-300d-1M', dest_dir='.') # Download fasttext model
#
#
# # model_type: word2vec, glove or fasttext
# aug = naw.WordEmbsAug(
#     model_type='word2vec', model_path=os.environ["MODEL_DIR"]+'GoogleNews-vectors-negative300.bin',
#     action="insert")
# augmented_text = aug.augment(text)
# print("Original:")
# print(text)
# print("Augmented Text:")
# print(augmented_text)

